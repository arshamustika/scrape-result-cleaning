{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582c5ce0-ec58-489c-a52c-45c329aa89c4",
   "metadata": {
    "id": "582c5ce0-ec58-489c-a52c-45c329aa89c4"
   },
   "source": [
    "# From Scraped Text to Analysis-Ready Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca5de1-16cd-4e69-a4cf-90067afd04ac",
   "metadata": {
    "id": "2dca5de1-16cd-4e69-a4cf-90067afd04ac"
   },
   "source": [
    "This code was made as a part of course group assignment to build corpus collection. To collect human-written news articles, we used webscrapper.io. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e12b91-9919-48bf-bbb0-8c3c44a6f989",
   "metadata": {
    "id": "95e12b91-9919-48bf-bbb0-8c3c44a6f989"
   },
   "source": [
    "Since our topic is about Jacinda Ardern resignation, we searched the news articles using Google Search. We only use \"ardern\" as our search keyword and did not specify \"resignation\", \"resign\", \"quit\" or other word implying her resignation to broaden our search so we can also get the news related to her and then we can filter later. However, we limit our source to one or two outlets. In this case, we used Tools > Advanced Search. And then we put the news site URL to the 'site or domain' field, e.g. https://www.rnz.co.nz/ and https://www.nzherald.co.nz/. We also narrow the results to only use English language. After that, we set the date range of the news article to focus the search in January, which was when the resignation took place. Although apparently we also got the results outside of the date range we set, however we still got most of the news articles in January just as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41628de1-90a3-4527-8705-70e16fca9508",
   "metadata": {
    "id": "41628de1-90a3-4527-8705-70e16fca9508"
   },
   "source": [
    "After the Google Search parameters set, we then use scraper tool webscrapper.io with the url in the url address field as the root.\n",
    "We set the selector and we also set the story_text as SelectorGroup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba95593-2982-4274-bbc7-c68e1fcb4686",
   "metadata": {
    "id": "0ba95593-2982-4274-bbc7-c68e1fcb4686"
   },
   "source": [
    "Once the scrape process finished, we export the result to csv. However, since we grouped the text paragraphs, we got our story_text column in json format.\n",
    "Therefore, we need to clean it. To clean the article content, we use a python script.\n",
    "Beside cleaning the content, we also use this script to only select the story text, exclude the urls and extra informations that we do not need\n",
    "and save the result in text files.\n",
    "Each row in the csv results one text file which file name is from notation, article date and the article title which trimmed to 30 characters.\n",
    "The trimming is to avoid the issue when we extract the zipped text files due to filename is too long.\n",
    "\n",
    "Below is the python script we used for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e767a-9f87-4c42-8414-fb9a982b4861",
   "metadata": {
    "id": "848e767a-9f87-4c42-8414-fb9a982b4861"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(filename):\n",
    "    # Replace spaces and special characters with underscores\n",
    "    return re.sub(r'[<>:\\'\"/\\\\|?*]', '_', filename).replace(' ', '_')[:30].ljust(30)\n",
    "\n",
    "# Function to set the filename from notation, date and title\n",
    "def set_filename(notation, date_str, sanitized_title):\n",
    "    # get the date\n",
    "    # since rnz and nz herald use different date format in their articles\n",
    "    date_object = datetime.strptime(date_str.strip(), \"%I:%M %p on %d %B %Y\") if (notation == \"rnz\") else datetime.strptime(date_str.strip(), '%d %b, %Y %I:%M %p') # else nzherald\n",
    "    formatted_date = date_object.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return f\"{notation} {formatted_date} {filename}\"\n",
    "\n",
    "# Function to convert the json format of the story_text to plain text\n",
    "def clean_text(json_str):\n",
    "    # Initialize a list to hold the values\n",
    "    story_text_arr = []\n",
    "\n",
    "    json_array = json.loads(json_str)\n",
    "\n",
    "    # Extract values from each JSON object\n",
    "    for obj in json_array:\n",
    "        # Example: Extract the 'story_text' value from each JSON object\n",
    "        paragraph = obj.get('story_text')\n",
    "        story_text.append(paragraph)\n",
    "\n",
    "    return story_text_arr\n",
    "\n",
    "# use these for rnz, and comment the nzherald part\n",
    "notation = '[rnz]'\n",
    "csv_file_path = 'googleArdern.csv' # Path to the input CSV file\n",
    "output_directory = 'googleArdern_rnz_output_text_only'  # Directory where text files will be saved\n",
    "\n",
    "# use these for nzherald, and comment the rnz part\n",
    "# notation = '[nzherald]'\n",
    "# csv_file_path = 'google_nzherald.csv' # Path to the input CSV file\n",
    "# output_directory = 'google_nzherald_output_text_only'  # Directory where text files will be saved\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "\n",
    "    # Process each row in the CSV file\n",
    "    for row_index, row in enumerate(csv_reader):\n",
    "        if len(row) >= 7 and row[5].strip() != '' and 'Ardern' in row[6]: # filter only when text contains 'ardern'\n",
    "            filename_base = sanitize_filename(row[4].strip()) # Get the title from the 5th column (index 4)\n",
    "            filename = set_filename(notation, row[5], filename_base)\n",
    "\n",
    "            # Create a file name for each row\n",
    "            text_file_name = f'{filename}.txt'\n",
    "            text_file_path = os.path.join(output_directory, text_file_name)\n",
    "\n",
    "            # get the story text\n",
    "            story_text = clean_text(row[6])\n",
    "\n",
    "            # Open the text file for writing\n",
    "            with open(text_file_path, mode='w', encoding='utf-8') as txt_file:\n",
    "                row_text = '\\n\\n'.join(str(x) for x in story_text)\n",
    "                txt_file.write(row_text + '\\n')\n",
    "\n",
    "print(f\"Each row has been written to a separate text file in '{output_directory}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efdcd4-d764-45be-a5c1-a98591c01283",
   "metadata": {
    "id": "81efdcd4-d764-45be-a5c1-a98591c01283"
   },
   "source": [
    "After running the script, we got the cleaned text files as a result from scraping from rnz and nzherald sites."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
